# Horizontal Pod Autoscaler for LLM Config Manager
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-config-manager
  namespace: llm-config
  labels:
    app: llm-config-manager
    component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-config-manager

  minReplicas: 3
  maxReplicas: 10

  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale when CPU > 70%

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80  # Scale when memory > 80%

    # Custom metrics (requires metrics-server and custom metrics adapter)
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"  # Scale when RPS > 1000 per pod

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Percent
          value: 50  # Scale down max 50% of pods at a time
          periodSeconds: 60
        - type: Pods
          value: 2  # Scale down max 2 pods at a time
          periodSeconds: 60
      selectPolicy: Min  # Use the policy that scales down the least

    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
        - type: Percent
          value: 100  # Scale up max 100% of pods at a time
          periodSeconds: 15
        - type: Pods
          value: 4  # Scale up max 4 pods at a time
          periodSeconds: 15
      selectPolicy: Max  # Use the policy that scales up the most
